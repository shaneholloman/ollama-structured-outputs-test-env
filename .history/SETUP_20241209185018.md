# Ollama Structured Outputs Setup

## JavaScript Implementation

### Environment Setup

1. **Node.js Environment**
   - Used Volta for Node.js version management
   - Pinned Node.js version 20 using `volta pin node@20`
   - Installed npm via Volta: `volta install npm`

2. **Dependencies**
   - ollama: ^0.5.11 (Ollama JavaScript client)
   - zod: ^3.23.8 (Runtime type checking)
   - zod-to-json-schema: ^3.23.5 (Converts Zod schemas to JSON Schema)

### Implementation Details

#### Schema Definition
```javascript
const City = z.object({
    name: z.string(),
    country: z.string(),
});

const Cities = z.object({
    cities: z.array(City)
})
```

### Ollama Integration
- Model: llama3.2:latest
- Using structured format output with JSON Schema
- Query: List of 5 cities from around the world and their countries

### Example Output
```javascript
{
  cities: [
    { name: 'Paris', country: 'France' },
    { name: 'Tokyo', country: 'Japan' },
    { name: 'Rio de Janeiro', country: 'Brazil' },
    { name: 'Beijing', country: 'China' },
    { name: 'Cape Town', country: 'South Africa' }
  ]
}
```

### Key Features
1. **Type Safety**: Using Zod for runtime type validation
2. **Structured Data**: Consistent JSON output format
3. **Schema Validation**: Automatic validation of LLM responses
4. **ESM Support**: Using ES modules for modern JavaScript

### Running the JavaScript Version
1. Ensure Ollama is installed and running
2. Pull the required model: `ollama pull llama3.2:latest`
3. Run the script: `node ollama_only.js`

## Python Implementations

### Environment Setup

1. **Python Environment**
   - Python 3.12 (configured via .python-version)
   - Using uv package manager for dependency management

2. **Dependencies** (installed via `uv pip install .`)
   - ollama: 0.4.4
   - instructor: 1.7.0
   - pydantic: 2.10.3
   - openai: 1.57.0
   - requests (for direct API calls)

### Implementation 1: Using Ollama Package

This implementation uses the official Ollama Python package with Pydantic for type validation.

```python
from typing import List
from ollama import chat
from pydantic import BaseModel

class City(BaseModel):
    name: str
    country: str

class Cities(BaseModel):
    cities: List[City]

response = chat(
    model="llama3.2:latest",
    messages=[{"role": "user", "content": "List 5 cities..."}],
    format=Cities.model_json_schema(),
)

city_list = Cities.model_validate_json(response.message.content)
```

### Implementation 2: Using Instructor

This implementation uses the Instructor package which provides an OpenAI-compatible interface.

```python
from typing import List
from openai import OpenAI
from pydantic import BaseModel
import instructor

class City(BaseModel):
    name: str
    country: str

class Cities(BaseModel):
    cities: List[City]

client = instructor.from_openai(
    OpenAI(
        base_url="http://localhost:11434/v1",
        api_key="ollama",
    ),
    mode=instructor.Mode.JSON,
)

city_list = client.chat.completions.create(
    model="llama2",
    messages=[{"role": "user", "content": "List 5 cities..."}],
    response_model=Cities,
)
```

### Implementation 3: Direct API Calls

This implementation makes direct HTTP calls to Ollama's API endpoint.

```python
import requests
import json

response = requests.post('http://localhost:11434/api/generate', json={
    "model": "llama2",
    "prompt": "List 5 cities...",
    "stream": False,
    "format": "json"
})

result = json.loads(response.content)['response']
```

### Key Features Across Python Implementations

1. **Multiple Approaches**:
   - Official Ollama package with Pydantic
   - Instructor with OpenAI-compatible interface
   - Direct API calls for low-level control

2. **Type Safety**:
   - Pydantic models for data validation
   - Runtime type checking
   - Structured response handling

3. **Flexibility**:
   - Choose implementation based on needs
   - All methods support JSON structured output
   - Various levels of abstraction available

### Running the Python Versions

1. Ensure Ollama is installed and running
2. Install dependencies: `uv pip install .`
3. Run any of the implementations:
   - `python ollama_only.py`
   - `python instructor_ollama.py`
   - `python ollama_json.py`
