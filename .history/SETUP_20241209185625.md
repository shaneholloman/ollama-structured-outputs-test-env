# Ollama Structured Outputs Setup

## JavaScript Implementation

### Environment Setup

1. **Node.js Environment**
   - Used Volta for Node.js version management
   - Pinned Node.js version 20 using `volta pin node@20`
   - Installed npm via Volta: `volta install npm`

2. **Dependencies**
   - ollama: ^0.5.11 (Ollama JavaScript client)
   - zod: ^3.23.8 (Runtime type checking)
   - zod-to-json-schema: ^3.23.5 (Converts Zod schemas to JSON Schema)

### Implementation Details

#### Schema Definition

```javascript
const City = z.object({
    name: z.string(),
    country: z.string(),
});

const Cities = z.object({
    cities: z.array(City)
})
```

### Ollama Integration

- Model: llama3.2:latest
- Using structured format output with JSON Schema
- Query: List of 5 cities from around the world and their countries

### Example Output

```javascript
{
  cities: [
    { name: 'Paris', country: 'France' },
    { name: 'Tokyo', country: 'Japan' },
    { name: 'Rio de Janeiro', country: 'Brazil' },
    { name: 'Beijing', country: 'China' },
    { name: 'Cape Town', country: 'South Africa' }
  ]
}
```

### Key Features

1. **Type Safety**: Using Zod for runtime type validation
2. **Structured Data**: Consistent JSON output format
3. **Schema Validation**: Automatic validation of LLM responses
4. **ESM Support**: Using ES modules for modern JavaScript

### Running the JavaScript Version

1. Ensure Ollama is installed and running
2. Pull the required model: `ollama pull llama3.2:latest`
3. Run the script: `node ollama_only.js`

## Python Implementations

### Environment Setup

1. **Python Environment**
   - Python 3.12 (configured via .python-version)
   - Using uv package manager for dependency management

2. **Dependencies** (installed via `uv pip install .`)
   - ollama: 0.4.4
   - instructor: 1.7.0
   - pydantic: 2.10.3
   - openai: 1.57.0
   - requests (for direct API calls)

### Implementation 1: Using Ollama Package

This implementation uses the official Ollama Python package with Pydantic for type validation.

```python
from typing import List
from ollama import chat
from pydantic import BaseModel

class City(BaseModel):
    name: str
    country: str

class Cities(BaseModel):
    cities: List[City]

response = chat(
    model="llama3.2:latest",
    messages=[{"role": "user", "content": "List 5 cities..."}],
    format=Cities.model_json_schema(),
)

city_list = Cities.model_validate_json(response.message.content)
```

### Implementation 2: Using Instructor

This implementation uses the Instructor package which provides an OpenAI-compatible interface.

```python
from typing import List
from openai import OpenAI
from pydantic import BaseModel
import instructor

class City(BaseModel):
    name: str
    country: str

class Cities(BaseModel):
    cities: List[City]

client = instructor.from_openai(
    OpenAI(
        base_url="http://localhost:11434/v1",
        api_key="ollama",
    ),
    mode=instructor.Mode.JSON,
)

city_list = client.chat.completions.create(
    model="llama3.2:latest",
    messages=[{"role": "user", "content": "List 5 cities..."}],
    response_model=Cities,
)
```

### Implementation 3: Direct API Calls

This implementation makes direct HTTP calls to Ollama's API endpoint. Note that it requires additional transformation of the response to match our desired format.

```python
import requests
import json

response = requests.post('http://localhost:11434/api/chat', json={
    "model": "llama3.2:latest",
    "messages": [
        {
            "role": "user",
            "content": "List 5 cities..."
        }
    ],
    "format": "json",
    "stream": False
})

# The API returns a different format that needs transformation
result = json.loads(response.content)
content_json = json.loads(result['message']['content'])

# Transform into our desired format
cities_list = {
    "cities": [
        {"name": name.strip(), "country": country.strip()}
        for name, country in content_json.items()
    ]
}
```

Example raw API response format:

```json
{
  "model": "llama3.2:latest",
  "message": {
    "role": "assistant",
    "content": "{\n \"London\" : \"United Kingdom\",\n \"Tokyo\" : \"Japan\", \n \"New York City\" : \"United States\", \n \"Paris\" : \"France\", \n \"Sydney\" : \"Australia\"\n}"
  }
}
```

Transformed to match our schema:

```json
{
  "cities": [
    {"name": "London", "country": "United Kingdom"},
    {"name": "Tokyo", "country": "Japan"},
    {"name": "New York City", "country": "United States"},
    {"name": "Paris", "country": "France"},
    {"name": "Sydney", "country": "Australia"}
  ]
}
```

### Key Features Across Python Implementations

1. **Multiple Approaches**:
   - Official Ollama package with Pydantic: Most straightforward with built-in schema validation
   - Instructor with OpenAI-compatible interface: Familiar for those using OpenAI
   - Direct API calls: Lower level control but requires response transformation

2. **Type Safety**:
   - Pydantic models for data validation
   - Runtime type checking
   - Structured response handling

3. **Response Formats**:
   - Ollama package & Instructor: Direct schema-validated output
   - Direct API: Requires transformation from raw response

### Running the Python Versions

1. Ensure Ollama is installed and running
2. Install dependencies: `uv pip install .`
3. Run any of the implementations:
   - `python ollama_only.py`
   - `python instructor_ollama.py`
   - `python ollama_json.py`
